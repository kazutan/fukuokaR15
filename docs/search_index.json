[
["index.html", "データハンドリングFAQ v0.1 はじめに 参考資料", " データハンドリングFAQ v0.1 前田和寛(@kazutan) 2019-08-31 はじめに これはfukuoka.R #15 にて同タイトルで発表した内容です。本編は次のページ以降になります 参考資料 WIP… "],
["01_case_one.html", "1 Case 1: multi-gather/spread問題 1.1 Question 1 1.2 Answer 1.3 解説 1.4 参考資料", " 1 Case 1: multi-gather/spread問題 1.1 Question 1 以下のようなデータがあります: # 資料を読んでる人が再現できるように、データ生成用コードも残しときます n = 30 df_1 &lt;- data.frame( id = rep(1:10, each = 3), group = rep(letters[1:3], 10), x_pre = round(rnorm(n) * 100), x_post = round(rnorm(n, 1, 1) * 100), y_pre = round(rnorm(n, 2, 3) * 100), y_post = round(rnorm(n, 3, 1) * 100) ) knitr::kable(head(df_1)) id group x_pre x_post y_pre y_post 1 a 26 117 514 260 1 b 79 20 740 306 1 c -44 169 477 358 2 a 147 147 324 415 2 b 332 50 -171 418 2 c 58 121 228 431 これを、以下のようにしたいです: id pre_post a_x a_y b_x b_y c_x c_y 1 post 117 260 20 306 169 358 1 pre 26 514 79 740 -44 477 2 post 147 415 50 418 121 431 2 pre 147 324 332 -171 58 228 3 post 146 226 7 367 168 256 3 pre 87 218 95 333 29 -288 どうしたらいいのでしょうか? 1.2 Answer 以下のようにやります: library(tidyverse) df_1_result &lt;-df_1 %&gt;% gather(key = var_name, value = value, -c(id, group)) %&gt;% separate(var_name, c(&quot;var&quot;, &quot;pre_post&quot;)) %&gt;% unite(new_var, group, var) %&gt;% spread(key = new_var, value = value) knitr::kable(head(df_1_result)) id pre_post a_x a_y b_x b_y c_x c_y 1 post 117 260 20 306 169 358 1 pre 26 514 79 740 -44 477 2 post 147 415 50 418 121 431 2 pre 147 324 332 -171 58 228 3 post 146 226 7 367 168 256 3 pre 87 218 95 333 29 -288 1.3 解説 1.3.1 考え方 通称｢multi-gather, multi-spread問題｣の一種です。 wide-long変換をするにはtidyr::gatherやtidyr::spreadを使えばいいのですが、今回は単純にそれらを使うだけではうまく行きません。そこで以下のようなアプローチをします: 一旦tidyなデータ(long data)に整形 変数名を切り離す 目的の変数名を作成 新たに作った変数名をkeyにしてwideに展開 1.3.2 手順 まずはgather: res &lt;- df_1 %&gt;% gather(key = var_name, value = value, -c(id, group)) knitr::kable(head(res)) id group var_name value 1 a x_pre 26 1 b x_pre 79 1 c x_pre -44 2 a x_pre 147 2 b x_pre 332 2 c x_pre 58 ここからがポイントで、当初の変数名を2つに切り離します: res &lt;- res %&gt;% separate(var_name, c(&quot;var&quot;, &quot;pre_post&quot;)) knitr::kable(head(res)) id group var pre_post value 1 a x pre 26 1 b x pre 79 1 c x pre -44 2 a x pre 147 2 b x pre 332 2 c x pre 58 これで要素がちゃんと分かれたデータになりました。そして目的の変数名になるようひっつけます: res &lt;- res %&gt;% unite(new_var, group, var) knitr::kable(head(res)) id new_var pre_post value 1 a_x pre 26 1 b_x pre 79 1 c_x pre -44 2 a_x pre 147 2 b_x pre 332 2 c_x pre 58 あとはこの変数名の列をkeyとしてwideにします: res &lt;- res %&gt;% spread(key = new_var, value = value) knitr::kable(head(res)) id pre_post a_x a_y b_x b_y c_x c_y 1 post 117 260 20 306 169 358 1 pre 26 514 79 740 -44 477 2 post 147 415 50 418 121 431 2 pre 147 324 332 -171 58 228 3 post 146 226 7 367 168 256 3 pre 87 218 95 333 29 -288 これでOKです。 1.3.3 応用 今回はvalueにあたるデータが全て数値だったのでスムーズでしたが、型が違う場合もあります: n = 30 df_1a &lt;- data.frame( id = rep(1:10, each = 3), group = rep(letters[1:3], 10), x_pre = round(rnorm(n) * 100), x_post = round(rnorm(n, 1, 1) * 100), y_pre = sample(c(&quot;kosaki&quot;, &quot;chitoge&quot;), n, replace = TRUE, prob = c(5, 5)), y_post = sample(c(&quot;kosaki&quot;, &quot;chitoge&quot;), n, replace = TRUE, prob = c(9, 1)) ) knitr::kable(head(df_1a)) id group x_pre x_post y_pre y_post 1 a 63 14 chitoge kosaki 1 b 115 57 kosaki kosaki 1 c -96 65 chitoge kosaki 2 a -115 233 kosaki kosaki 2 b -81 250 chitoge kosaki 2 c 43 151 kosaki chitoge この場合、まずは気にせずに同じように整形し、あとから列の型を変更すればOKです res_a &lt;- df_1a %&gt;% # このときvalueがcharacter型になる gather(key = var_name, value = value, -c(id, group)) %&gt;% # 気にせず処理 separate(var_name, c(&quot;var&quot;, &quot;pre_post&quot;)) %&gt;% unite(new_var, group, var) %&gt;% spread(new_var, value) %&gt;% # 数値にしたい列を変換 mutate_at(vars(ends_with(&quot;_x&quot;)), as.numeric) knitr::kable(head(res_a)) id pre_post a_x a_y b_x b_y c_x c_y 1 post 14 kosaki 57 kosaki 65 kosaki 1 pre 63 chitoge 115 kosaki -96 chitoge 2 post 233 kosaki 250 kosaki 151 chitoge 2 pre -115 kosaki -81 chitoge 43 kosaki 3 post -83 kosaki 56 kosaki 136 chitoge 3 pre -41 chitoge -22 kosaki 72 kosaki 以上です。なお個人的には全てtidyにしたいです。 1.4 参考資料 dplyr 関数リファレンス tidyr 関数リファレンス "],
["02_case_two.html", "2 Case 2: ファイル一気読み問題 2.1 Question 2 2.2 Answer 2.3 解説 2.4 参考資料", " 2 Case 2: ファイル一気読み問題 2.1 Question 2 あるディレクトリ内に、一つのファイルに1人分のデータが入ったcsvファイルがたくさんあります: set.seed(57) library(tidyverse) # ティレクトリ作成 if (!dir.exists(&quot;data&quot;)) { dir.create(&quot;data&quot;) } # CSVファイルを作成 # 中身は確認しといてください tmp_df &lt;- data.frame() for (i in 1:10) { path &lt;- paste(&quot;data/file_&quot;, LETTERS[i], &quot;.csv&quot;) tmp_df &lt;- sample_n(iris, sample(1:4, 1)) write.csv(tmp_df, path, row.names = FALSE) } これを一気にまとめて読み込んで、ひとつのデータにまとめたいです。どうしたら一番ラクでしょうか? また、後から確認できるように、どのファイルから持ってきたデータなのかの情報も加えたいです。 2.2 Answer こんな感じです: # csvが入っているディレクトリからCSVファイル名を取得 csv_fname &lt;- dir(&quot;data&quot;, full.names = TRUE) %&gt;% str_subset(&quot;\\\\.csv$&quot;) # 読み込み関数を定義 r_csv_w_fname &lt;- function(path) { if (file.exists(path)) { df &lt;- read_csv(path) if (nrow(df) &gt; 0) { df &lt;- mutate(df, fname = path) return(df) } } } # あとはこの1行でOK df &lt;- map_dfr(csv_fname, r_csv_w_fname) # 内容を確認 knitr::kable(sample_n(df, 10)) Sepal.Length Sepal.Width Petal.Length Petal.Width Species fname 5.4 3.4 1.5 0.4 setosa data/file_ J .csv 6.8 2.8 4.8 1.4 versicolor data/file_ A .csv 5.0 3.4 1.5 0.2 setosa data/file_ J .csv 6.3 2.5 5.0 1.9 virginica data/file_ J .csv 6.7 3.3 5.7 2.1 virginica data/file_ F .csv 5.2 3.4 1.4 0.2 setosa data/file_ E .csv 6.3 2.5 5.0 1.9 virginica data/file_ G .csv 6.7 3.3 5.7 2.5 virginica data/file_ G .csv 5.5 2.4 3.7 1.0 versicolor data/file_ E .csv 4.8 3.4 1.9 0.2 setosa data/file_ B .csv 2.3 解説 2.3.1 考え方 通称｢ファイル一気読み問題｣といわれるものです。今回は読み込んでつなげる上に、｢ファイル名の情報を追加しろ｣といわれています。あとから確認できるようにすることはとても大切です。 基本的な考え方は以下のとおりです: 読み込むファイルのバスを準備 パスにあるファイルを読み込む 読み込んだデータにファイル情報を付与 データを結合 それでは今回の内容について、順を追って説明します。 2.3.2 手順 まずはcsvファイルのパスを準備します: csv_fname &lt;- dir(&quot;data&quot;, full.names = TRUE) %&gt;% str_subset(&quot;\\\\.csv$&quot;) やり方は色々あるでしょうが、私はだいいたいこんな感じでやります。 stringr::str_subset() は文字列ベクトルからパターンにマッチした文字列を残します。また、ファイル名だけではパスとして不十分なので、dir関数のfull.names引数でフルパスを取得するようにしています。 ｢読み込んでデータを加工する｣を繰り返すのでforループしかないと思うかもしれませんが、自分で関数を定義して準備するといいでしょう: # 読み込み関数を定義 r_csv_w_fname &lt;- function(path) { if (file.exists(path)) { df &lt;- read_csv(path) if (nrow(df) &gt; 0) { df &lt;- mutate(df, fname = path) return(df) } } } 内容はシンプルなので問題ないかとは思います。なおifをつけなくても今回のは動くのですが、ある程度は対処していた方がいいです。あと、データハンドリングでは自作関数を準備する場面がかなり多いです。 関数を準備したので、あとはこの関数にパスを順次送り込んで、返り値を行方向に結合していけばOKです。Rのbaseにはapplyがありますが、今回は purrr::map_dfrが断然楽です: df &lt;- map_dfr(csv_fname, r_csv_w_fname) map_dfr関数はmap -&gt; as.data.frame -&gt; bind_rows というのを一気にやってくれるイメージです。なお、pam_dfrにはbind_rowsと同じく.id`引数があるので、｢単純にどのファイルからやってきているかさえ識別できればいい｣のであれば、関数を定義せずにこれだけでもいいと思います。 2.3.3 応用 もしファイルがcsvではなくExcelファイルなどである場合は、read_csvではなく他の読み込み関数を使えばOKです。 また、自作関数内で処理を加えれば、いろいろなことができるでしょう。 2.4 参考資料 purrr関数リファレンス "],
["03_case_three.html", "3 Case 3: 特定期間の指定･切り出し問題 3.1 想定シナリオ 3.2 処理の実行 3.3 参照", " 3 Case 3: 特定期間の指定･切り出し問題 ここでは、lesson2で準備したdf_logという仮想ログデータを用いて、一定期間での集計をまとめることを目指します。その中でlubridateのintervalオブジェクトと%within%演算子を説明します。 3.1 想定シナリオ lesson2で作成したアイテム購入ログデータを元に、1/16から1/31までを曜日別集計を行います: 期間内購入件数 期間内売上合計 期間内購入単価平均 この集計により、その期間での売上を検証することができます。 また、特定の期間とそれ以外の期間での比較を行います: 購入件数の比較 売上合計の比較 これにより、期間別の比較をすることができます。 3.2 処理の実行 3.2.1 パッケージ読み込み ここで使用するパッケージを読み込みます: library(tidyverse) library(lubridate) library(gridExtra) 3.2.2 データ読み込み lesson2で作成したcsvを読み込みます。readr::read_csv()を使います: df_log &lt;- read_csv(&quot;df_log.csv&quot;) 3.2.3 データハンドリングと可視化 3.2.3.1 特定期間を取り出す 特定の期間でどれだけ売上があったかを集計してデータセットにします。基本的な流れは以下のとおりです: 特定期間を表すietervalオブジェクトを作成 特定期間のデータを抽出 各種集計を実施 実際のRのコードは以下のようになります: # intervalオブジェクトを生成 target_interval &lt;- interval( start = ymd(&quot;2018-1-16&quot;), end = ymd(&quot;2018-1-31&quot;) ) # ターゲット期間のみを取り出して集計 # 指定が効いているのを確認するため日別集計 df_log_interval &lt;- df_log %&gt;% filter(stamp %within% target_interval) %&gt;% mutate(date = date(stamp)) %&gt;% group_by(date) %&gt;% summarise( n_buy = n(), value_buy = sum(value), mean_buy = mean(value) ) この処理を行ったデータセットは以下のようになります: knitr::kable(head(df_log_interval, 10)) date n_buy value_buy mean_buy 2018-01-16 195 73500 376.9231 2018-01-17 191 67600 353.9267 2018-01-18 204 68700 336.7647 2018-01-19 186 67000 360.2151 2018-01-20 188 79200 421.2766 2018-01-21 196 73400 374.4898 2018-01-22 199 70900 356.2814 2018-01-23 216 85200 394.4444 2018-01-24 188 58400 310.6383 2018-01-25 217 74900 345.1613 このデータを元にggplot2で可視化してみます: # 指定区間を取り出した範囲で可視化 p_int_date &lt;- ggplot(df_log_interval) p_int_date1 &lt;- p_int_date + geom_line(aes(x = date, y = n_buy)) p_int_date2 &lt;- p_int_date + geom_line(aes(x = date, y = value_buy)) p_int_date3 &lt;- p_int_date + geom_line(aes(x = date, y = mean_buy)) grid.arrange(p_int_date1, p_int_date2, p_int_date3, nrow = 2) ついでに、指定期間との比較をやってみます # 区間指定の応用。一定期間とそれ以外で検証 # filterではなくif_elseでmutateする # 3つ以上ならcase_whenでパターンを準備すればOK df_log_interval_comp &lt;- df_log %&gt;% mutate(target = if_else( stamp %within% target_interval, &quot;target&quot;, &quot;other&quot; )) %&gt;% mutate(hour = hour(stamp)) %&gt;% group_by(hour, target) %&gt;% summarise( n_buy = n(), value_buy = sum(value), mean_buy = mean(value) ) この処理を行ったデータセットは以下のようになります: knitr::kable(head(df_log_interval_comp, 10)) hour target n_buy value_buy mean_buy 0 other 306 108500 354.5752 0 target 129 43100 334.1085 1 other 293 112300 383.2765 1 target 111 41000 369.3694 2 other 307 112300 365.7980 2 target 127 52600 414.1732 3 other 277 110000 397.1119 3 target 121 41200 340.4959 4 other 290 113300 390.6897 4 target 139 48900 351.7986 このデータを元にggplot2で可視化してみます: p_int_comp_date &lt;- ggplot(df_log_interval_comp) p_int_comp_date1 &lt;- p_int_comp_date + geom_boxplot(aes(x = target, y = mean_buy, color = target)) + coord_flip() p_int_comp_date2 &lt;- p_int_comp_date + geom_line(aes(x = hour, y = mean_buy, color = target)) grid.arrange(p_int_comp_date1, p_int_comp_date2, nrow = 2) 3.2.4 解説 今回のポイントは、「この日時からこの日時まで」という特定の期間を示すintervalオブジェクトです。 lubridateパッケージには、独自でintervalクラスが実装されています: # 使う日時データを準備 x1 &lt;- ymd_hms(&quot;2018-02-23 17:50:00&quot;) x2 &lt;- ymd_hms(&quot;2018-02-25 17:50:00&quot;) # intervalオブジェクトを作成 x_i &lt;- interval( start = x1, end = x1 + days(1) ) # そのまま出してみる x_i #&gt; [1] 2018-02-23 17:50:00 UTC--2018-02-24 17:50:00 UTC # 開始日時を取得 int_start(x_i) #&gt; [1] &quot;2018-02-23 17:50:00 UTC&quot; # 終了日時を取得 int_end(x_i) #&gt; [1] &quot;2018-02-24 17:50:00 UTC&quot; # 時間の幅を取得 int_length(x_i) #&gt; [1] 86400 # interval作成用の%--%演算子 x1 %--% x2 #&gt; [1] 2018-02-23 17:50:00 UTC--2018-02-25 17:50:00 UTC intervalオブジェクトは時間的な幅を秒単位で保持し、それに属性(attribute)で開始日時とタイムゾーン、そしてclass情報を付与しています: # 中身を強制的に出力 # 1日は86400秒 cat(x_i) #&gt; 86400 # 属性を出力してみる attributes(x_i) #&gt; $start #&gt; [1] &quot;2018-02-23 17:50:00 UTC&quot; #&gt; #&gt; $tzone #&gt; [1] &quot;UTC&quot; #&gt; #&gt; $class #&gt; [1] &quot;Interval&quot; #&gt; attr(,&quot;package&quot;) #&gt; [1] &quot;lubridate&quot; このあたりが理解できていれば、lesson2-4の内容を応用していろいろいじれることはわかるかと思います。このintervalオブジェクトに関する便利な関数がlubridateパッケージに準備してあります: # intervalを準備 x_i1 &lt;- interval(ymd(&quot;2018-01-01&quot;), ymd(&quot;2018-01-31&quot;)) x_i2 &lt;- interval(ymd(&quot;2018-01-16&quot;), ymd(&quot;2018-02-16&quot;)) x_i3 &lt;- interval(ymd(&quot;2018-02-01&quot;), ymd(&quot;2018-02-28&quot;)) # intervalをずらす # 第二引数には時間的な幅(duration(), days()など) int_shift(x_i1, days(5)) #&gt; [1] 2018-01-06 UTC--2018-02-05 UTC # 開始と終了を反転 x_i1_flip &lt;- int_flip(x_i1) x_i1_flip #&gt; [1] 2018-01-31 UTC--2018-01-01 UTC # 開始と終了を整える(順番をきれいにする) int_standardize(x_i1_flip) #&gt; [1] 2018-01-01 UTC--2018-01-31 UTC # 2つの区間が重なっているかどう(オーバーラップ)か判定 # これはオーバーラップしているのでTRUE int_overlaps(x_i1, x_i2) #&gt; [1] TRUE # これはオーバーラップしてないのでFALSE int_overlaps(x_i1, x_i3) #&gt; [1] FALSE # 2つのintervalで、開始and/or終了が同じかどうか(align)判定 # 説明用に新たにintervalを準備 x_i4 &lt;- interval(ymd(&quot;2018-01-01&quot;), ymd(&quot;2018-02-16&quot;)) # x_i4の開始はx_i1の開始と揃っているので、これはTRUE int_aligns(x_i4, x_i1) #&gt; [1] TRUE # x_i4の終了はx_i2の終了と揃っているので、これもTRUE int_aligns(x_i4, x_i2) #&gt; [1] TRUE # x_i4はx_i3の開始とも終了とも揃っていないので、これはFALSE int_aligns(x_i4, x_i3) #&gt; [1] FALSE # 日時ベクトルの要素で、その差分を利用してintervalベクトルを作成 # 説明用に日付のベクトルを生成 dates &lt;- now() + days(1:3) dates #&gt; [1] &quot;2019-09-01 11:50:16 JST&quot; &quot;2019-09-02 11:50:16 JST&quot; #&gt; [3] &quot;2019-09-03 11:50:16 JST&quot; # 差分でintervalベクトルを生成 # この場合、3つの日時から区間をつくるので長さが2になる int_diff(dates) #&gt; [1] 2019-09-01 11:50:16 JST--2019-09-02 11:50:16 JST #&gt; [2] 2019-09-02 11:50:16 JST--2019-09-03 11:50:16 JST また、「あるinterval/日時が、ある区間に含まれるかどうか」を判定する演算子として、lubridateには%within%が準備してあります。 # 説明用にintervalを準備 x_i1 &lt;- interval(ymd(&quot;2018-01-01&quot;), ymd(&quot;2018-01-31&quot;)) x_i2 &lt;- interval(ymd(&quot;2018-01-16&quot;), ymd(&quot;2018-01-31&quot;)) x_i3 &lt;- interval(ymd(&quot;2018-01-16&quot;), ymd(&quot;2018-02-16&quot;)) # %within%演算子のテスト # これは含まれるのでTRUE ymd(&quot;2018-01-10&quot;) %within% x_i1 #&gt; [1] TRUE # これは含まれないのでFALSE ymd(&quot;2018-02-10&quot;) %within% x_i1 #&gt; [1] FALSE # intervalでの比較 # これは含まれるのでTRUE x_i2 %within% x_i1 #&gt; [1] TRUE # これは(オーバーラップしてるけけど)内包しているわけではないのでFALSE x_i3 %within% x_i1 #&gt; [1] FALSE # この演算子は「左が右に含まれるかどうか」を判定 # なので、これはFALSE x_i1 %within% x_i2 #&gt; [1] FALSE 演算子の右にはintervalオブジェクトしか使えませんので注意してください。 3.3 参照 Utilities for creation and manipulation of Interval objects intervalオブジェクトを作成・操作する関数の説明 Interval class intervalクラスの説明 Tests whether a date or interval falls within an interval %within%演算子の説明 "],
["04_case_four.html", "4 Case 4: Nested df問題 4.1 Question 4 4.2 Answer 4.3 解説 4.4 参考資料", " 4 Case 4: Nested df問題 4.1 Question 4 以下のようなデータがあります: set.seed(57) library(tidyverse) library(lubridate) library(prophet) start_date = &quot;2019-01-01&quot; n = 100 df_4 &lt;- data.frame( yyyymmdd = seq(date(start_date), date(start_date) + days(n - 1), by = &quot;day&quot;), y_1 = sin(n) * 10 + rnorm(n), y_2 = sin(n) * 3 + rnorm(n, 2, 0.01), y_3 = cos(n) * 8 + rnorm(n) ) knitr::kable(head(df_4)) yyyymmdd y_1 y_2 y_3 2019-01-01 -5.757423 0.4802741 8.615518 2019-01-02 -6.830485 0.4812141 5.996507 2019-01-03 -4.441120 0.4799585 6.123853 2019-01-04 -3.046980 0.4899672 7.284474 2019-01-05 -4.922723 0.4894491 6.583605 2019-01-06 -3.437993 0.4807437 9.632790 このy_*に対して、prophetで予測させて未来のデータもあわせて作成したいです: y_var ds yhat yhat_lower yhat_upper y y_1 2019-01-01 -4.988744 -6.498488 -3.384716 -5.757423 y_1 2019-01-02 -5.378460 -6.842081 -3.815985 -6.830485 y_1 2019-01-03 -5.113240 -6.610575 -3.731660 -4.441120 y_1 2019-01-04 -4.901305 -6.258661 -3.484424 -3.046980 y_1 2019-01-05 -4.663884 -6.151910 -3.336566 -4.922723 y_1 2019-01-06 -4.793852 -6.307509 -3.414466 -3.437993 y_1 2019-01-07 -4.998286 -6.480009 -3.494410 -3.673579 y_1 2019-01-08 -4.993733 -6.425913 -3.536153 -5.941644 y_1 2019-01-09 -5.383449 -6.830365 -4.008951 -6.092223 y_1 2019-01-10 -5.118228 -6.462090 -3.545616 -4.112209 4.2 Answer こんな感じでOKです: m &lt;- function(d) { # fitting model. ここは各自ががんばってください model &lt;- prophet(d) # forecast future &lt;- make_future_dataframe(model, periods = 7) forecast &lt;- predict(model, future) # 欲しい部分を抽出して加工 res &lt;- forecast %&gt;% mutate(ds = date(ds)) %&gt;% select(ds, yhat, yhat_lower, yhat_upper) %&gt;% right_join(d) # return return(res) } df_4_result &lt;- df_4 %&gt;% gather(key = y_var, value = y, -yyyymmdd) %&gt;% rename(ds = yyyymmdd) %&gt;% group_by(y_var) %&gt;% nest() %&gt;% mutate(fit = map(data, m)) %&gt;% select(y_var, fit) %&gt;% unnest() knitr::kable(head(df_4_result, 10)) y_var ds yhat yhat_lower yhat_upper y y_1 2019-01-01 -4.988744 -6.452807 -3.509133 -5.757423 y_1 2019-01-02 -5.378460 -6.912193 -4.018997 -6.830485 y_1 2019-01-03 -5.113240 -6.575139 -3.735088 -4.441120 y_1 2019-01-04 -4.901305 -6.342179 -3.391854 -3.046980 y_1 2019-01-05 -4.663884 -6.120531 -3.264734 -4.922723 y_1 2019-01-06 -4.793852 -6.252291 -3.396149 -3.437993 y_1 2019-01-07 -4.998286 -6.445208 -3.590837 -3.673579 y_1 2019-01-08 -4.993733 -6.497540 -3.644810 -5.941644 y_1 2019-01-09 -5.383449 -6.968824 -3.936416 -6.092223 y_1 2019-01-10 -5.118228 -6.627071 -3.736248 -4.112209 4.3 解説 4.3.1 考え方 典型的なnested-df案件で、purrr::mapが本領発揮するケースです。｢繰り返しやること｣は関数化して、それをpurrr::mapでまとめましょう。 流れは以下のとおりです: modelingと必要な値を抽出する処理を関数化 使うデータセットをtidyに nest化 nest化したデータのそれぞれに準備した関数を当て、その結果を新たな列としてmutate 必要な部分だけ取り出してunnest 4.3.2 手順 まずはモデリングして整形する関数を作成します: m &lt;- function(d) { # fitting model. ここは各自ががんばってください model &lt;- prophet(d) # forecast future &lt;- make_future_dataframe(model, periods = 7) forecast &lt;- predict(model, future) # 欲しい部分を抽出して加工 res &lt;- forecast %&gt;% mutate(ds = date(ds)) %&gt;% select(ds, yhat, yhat_lower, yhat_upper) %&gt;% right_join(d) # return return(res) } このケースではprophetで予測しています。prophet便利ですよね。関数を作成したら、まずはこの関数単体でちゃんと動くか、実際にデータを渡してテストしてください。なお、ここで関数化している理由はいろいろありますが、このようにモデリング処理を関数化することによってモデルを修正していくコストを減らずことが大きいです。実際モデリングは何度も何度も繰り返すので、メンテしやすくしておくのは大切です。 あとはデータフローとなります。nest化を行います: res &lt;- df_4 %&gt;% gather(key = y_var, value = y, -yyyymmdd) %&gt;% rename(ds = yyyymmdd) %&gt;% group_by(y_var) %&gt;% nest() res #&gt; # A tibble: 3 x 2 #&gt; y_var data #&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 y_1 &lt;tibble [100 × 2]&gt; #&gt; 2 y_2 &lt;tibble [100 × 2]&gt; #&gt; 3 y_3 &lt;tibble [100 × 2]&gt; group_byしてからnestすることで、データを層別化できます。一部を取り出してみると、こんな感じです: res$data[[2]] #&gt; # A tibble: 100 x 2 #&gt; ds y #&gt; &lt;date&gt; &lt;dbl&gt; #&gt; 1 2019-01-01 0.480 #&gt; 2 2019-01-02 0.481 #&gt; 3 2019-01-03 0.480 #&gt; 4 2019-01-04 0.490 #&gt; 5 2019-01-05 0.489 #&gt; 6 2019-01-06 0.481 #&gt; 7 2019-01-07 0.473 #&gt; 8 2019-01-08 0.485 #&gt; 9 2019-01-09 0.466 #&gt; 10 2019-01-10 0.484 #&gt; # … with 90 more rows あとは｢各行のdataに対してmodeling｣します。ポイントは｢新たな列として、関数処理した結果を追加する｣というイメージです: res &lt;- res %&gt;% mutate(fit = map(data, m)) 今回作成したmという関数は、実測値(y)と予測値(yhat)および予測の上限･下限を含むdata.frameを返します。そのため、以下のような感じになります: res #&gt; # A tibble: 3 x 3 #&gt; y_var data fit #&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 y_1 &lt;tibble [100 × 2]&gt; &lt;data.frame [100 × 5]&gt; #&gt; 2 y_2 &lt;tibble [100 × 2]&gt; &lt;data.frame [100 × 5]&gt; #&gt; 3 y_3 &lt;tibble [100 × 2]&gt; &lt;data.frame [100 × 5]&gt; res$fit[[2]] %&gt;% head() #&gt; ds yhat yhat_lower yhat_upper y #&gt; 1 2019-01-01 0.4773502 0.4649810 0.4897357 0.4802741 #&gt; 2 2019-01-02 0.4805435 0.4670079 0.4934888 0.4812141 #&gt; 3 2019-01-03 0.4819156 0.4696508 0.4946583 0.4799585 #&gt; 4 2019-01-04 0.4804376 0.4667566 0.4936465 0.4899672 #&gt; 5 2019-01-05 0.4870225 0.4744456 0.4993775 0.4894491 #&gt; 6 2019-01-06 0.4821182 0.4700115 0.4949034 0.4807437 あとは、これを普段使うdata.frameのような感じに展開したいのですが、これはunnestでOKです: res &lt;- res %&gt;% select(y_var, fit) %&gt;% unnest() 4.3.3 応用 今回のデータをy_varをkeyとして横に展開したい場合もあるでしょうが、これはすでにケース1で紹介しているので省略します。 また多くの場合、これをggplot2などでplotすると思います。これもnest化してやってしまいましょう: # plotする部分を関数化 f_p &lt;- function(df, var_name) { ggplot(df, aes(x = ds)) + # geom geom_ribbon(aes(ymin = yhat_lower, ymax = yhat_upper), color = &quot;#9999FF&quot;, fill = &quot;#9999FF99&quot;) + geom_line(aes(y = yhat), color = &quot;#0000FF&quot;) + geom_point(aes(y = y), color = &quot;#0000AA&quot;) + # scales scale_x_date(date_labels = &quot;%m-%d&quot;) + # theme &amp; labs theme_bw() + labs(title = var_name) } res &lt;- res %&gt;% group_by(y_var) %&gt;% nest() %&gt;% mutate(plot = map2(data, y_var, f_p)) res$plot %&gt;% gridExtra::marrangeGrob(nrow = 2, ncol = 2) 4.4 参考資料 purrr関数リファレンス prophet 本家Web site ggplot2 関数リファレンス "],
["05_case_five.html", "5 Case 5: window関数問題 5.1 Question 5 5.2 Answer 5.3 解説 5.4 応用 5.5 参考資料", " 5 Case 5: window関数問題 5.1 Question 5 以下のようなログデータがあります: library(tidyverse) library(lubridate) set.seed(57) n = 1000 df_5 &lt;- data.frame( user_id = 100000 + sample(1:50, n, replace = TRUE), shop_id = sample(paste(&quot;shop&quot;, str_pad(1:10, width = 2, pad = &quot;0&quot;), sep = &quot;_&quot;), n, replace = TRUE), dt = ymd_hms(&quot;2019-02-14 00:00:00&quot;) + days(sample(0:27, n, replace = TRUE)) + hours(sample(17:23, n, replace = TRUE)) + minutes(sample(0:59, n, replace = TRUE)) + seconds(sample(0:59, n, replace = TRUE)) ) knitr::kable(head(df_5)) user_id shop_id dt 100013 shop_09 2019-03-12 18:05:34 100026 shop_01 2019-03-01 18:09:00 100002 shop_08 2019-03-07 17:19:27 100009 shop_06 2019-02-23 18:57:06 100037 shop_10 2019-03-06 22:28:48 100034 shop_06 2019-02-14 17:38:08 ここで、shop_id == &quot;shop_05&quot;にはじめて訪問する前と後で、ユーザーが各店舗に何回訪問したかを集計したいです: user_id shop_id post pre 100001 shop_01 5 0 100001 shop_02 2 0 100001 shop_03 2 0 100001 shop_04 3 0 100001 shop_05 5 0 100001 shop_06 2 1 5.2 Answer いろいろやり方がありますが、こんな感じでもできます: df_5_result &lt;- df_5 %&gt;% arrange(user_id, dt) %&gt;% group_by(user_id) %&gt;% mutate(target_flag = if_else(shop_id == &quot;shop_05&quot;, 1, 0)) %&gt;% mutate(cum_target = cumsum(target_flag)) %&gt;% mutate(pre_post = if_else(cum_target == 0, &quot;pre&quot;, &quot;post&quot;)) %&gt;% group_by(user_id, shop_id, pre_post) %&gt;% summarise(count = n()) %&gt;% spread(pre_post, count, fill = 0) knitr::kable(head(df_5_result)) user_id shop_id post pre 100001 shop_01 5 0 100001 shop_02 2 0 100001 shop_03 2 0 100001 shop_04 3 0 100001 shop_05 5 0 100001 shop_06 2 1 5.3 解説 5.3.1 考え方 ログデータを前処理するときに頻出するパターンですが、その状況によってバリエーションが出てきます。このケースでは、フラグ立てとwindow関数の応用でいけます。 流れは以下のとおりです: user_id, dtで並べ替え user単位でgroup by ターゲットに訪問したレコード(行)にフラグ(1)を立てる ユーザー単位でフラグの累積和を算出 フラグ累積和が0(つまりまだターゲットに訪問していない)のレコードをpre、それ以外をpostとする あとはgroup byし直して普通に集計 ポイントは3-5です。SQLのノウハウでもよく共有されてるやり方をdplyrでトレースしてます。 5.3.2 手順 まずは並べ替えてgroup by: res &lt;- df_5 %&gt;% arrange(user_id, dt) %&gt;% group_by(user_id) knitr::kable(head(res)) user_id shop_id dt 100001 shop_06 2019-02-14 21:04:51 100001 shop_05 2019-02-14 22:52:52 100001 shop_05 2019-02-16 22:08:02 100001 shop_03 2019-02-17 19:45:45 100001 shop_07 2019-02-19 18:02:35 100001 shop_04 2019-02-20 20:50:28 今回の処理では、常にユーザー単位で処理をします。また、時系列で並べ替えておく必要もあるのでこの操作となります。 次に、ターゲットに訪問しているログを特定します: res &lt;- res %&gt;% mutate(target_flag = if_else(shop_id == &quot;shop_05&quot;, 1, 0)) knitr::kable(head(res, 20)) user_id shop_id dt target_flag 100001 shop_06 2019-02-14 21:04:51 0 100001 shop_05 2019-02-14 22:52:52 1 100001 shop_05 2019-02-16 22:08:02 1 100001 shop_03 2019-02-17 19:45:45 0 100001 shop_07 2019-02-19 18:02:35 0 100001 shop_04 2019-02-20 20:50:28 0 100001 shop_10 2019-02-22 19:41:16 0 100001 shop_03 2019-02-22 23:51:35 0 100001 shop_01 2019-02-24 20:27:48 0 100001 shop_09 2019-02-25 19:16:26 0 100001 shop_06 2019-02-26 21:50:13 0 100001 shop_01 2019-02-26 22:43:49 0 100001 shop_06 2019-02-27 21:23:47 0 100001 shop_09 2019-02-28 19:05:35 0 100001 shop_10 2019-03-04 19:23:41 0 100001 shop_05 2019-03-04 23:53:19 1 100001 shop_07 2019-03-04 23:53:47 0 100001 shop_05 2019-03-05 17:13:47 1 100001 shop_04 2019-03-06 20:44:25 0 100001 shop_04 2019-03-06 23:01:55 0 この後に、累積和を算出するdplyr::cumsumを利用します: res &lt;- res %&gt;% mutate(cum_target = cumsum(target_flag)) knitr::kable(head(res, 20)) user_id shop_id dt target_flag cum_target 100001 shop_06 2019-02-14 21:04:51 0 0 100001 shop_05 2019-02-14 22:52:52 1 1 100001 shop_05 2019-02-16 22:08:02 1 2 100001 shop_03 2019-02-17 19:45:45 0 2 100001 shop_07 2019-02-19 18:02:35 0 2 100001 shop_04 2019-02-20 20:50:28 0 2 100001 shop_10 2019-02-22 19:41:16 0 2 100001 shop_03 2019-02-22 23:51:35 0 2 100001 shop_01 2019-02-24 20:27:48 0 2 100001 shop_09 2019-02-25 19:16:26 0 2 100001 shop_06 2019-02-26 21:50:13 0 2 100001 shop_01 2019-02-26 22:43:49 0 2 100001 shop_06 2019-02-27 21:23:47 0 2 100001 shop_09 2019-02-28 19:05:35 0 2 100001 shop_10 2019-03-04 19:23:41 0 2 100001 shop_05 2019-03-04 23:53:19 1 3 100001 shop_07 2019-03-04 23:53:47 0 3 100001 shop_05 2019-03-05 17:13:47 1 4 100001 shop_04 2019-03-06 20:44:25 0 4 100001 shop_04 2019-03-06 23:01:55 0 4 この結果を見ればすぐにわかるかと思います。cumsumとかはwindow関数と呼ばれ、mutateの中で活用します。SQLだとover句をイメージしてもらえるとスムーズです。 このとき、0となっているのは｢まだフラグが立ってない時期(つまり訪問前)のログ｣となりますので、あとはこれを利用してpre-postラベルを準備します: res &lt;- res %&gt;% mutate(pre_post = if_else(cum_target == 0, &quot;pre&quot;, &quot;post&quot;)) knitr::kable(head(res, 20)) user_id shop_id dt target_flag cum_target pre_post 100001 shop_06 2019-02-14 21:04:51 0 0 pre 100001 shop_05 2019-02-14 22:52:52 1 1 post 100001 shop_05 2019-02-16 22:08:02 1 2 post 100001 shop_03 2019-02-17 19:45:45 0 2 post 100001 shop_07 2019-02-19 18:02:35 0 2 post 100001 shop_04 2019-02-20 20:50:28 0 2 post 100001 shop_10 2019-02-22 19:41:16 0 2 post 100001 shop_03 2019-02-22 23:51:35 0 2 post 100001 shop_01 2019-02-24 20:27:48 0 2 post 100001 shop_09 2019-02-25 19:16:26 0 2 post 100001 shop_06 2019-02-26 21:50:13 0 2 post 100001 shop_01 2019-02-26 22:43:49 0 2 post 100001 shop_06 2019-02-27 21:23:47 0 2 post 100001 shop_09 2019-02-28 19:05:35 0 2 post 100001 shop_10 2019-03-04 19:23:41 0 2 post 100001 shop_05 2019-03-04 23:53:19 1 3 post 100001 shop_07 2019-03-04 23:53:47 0 3 post 100001 shop_05 2019-03-05 17:13:47 1 4 post 100001 shop_04 2019-03-06 20:44:25 0 4 post 100001 shop_04 2019-03-06 23:01:55 0 4 post ここまでくれば、あとは集計です: res &lt;- res %&gt;% group_by(user_id, shop_id, pre_post) %&gt;% summarise(count = n()) knitr::kable(head(res, 10)) user_id shop_id pre_post count 100001 shop_01 post 5 100001 shop_02 post 2 100001 shop_03 post 2 100001 shop_04 post 3 100001 shop_05 post 5 100001 shop_06 post 2 100001 shop_06 pre 1 100001 shop_07 post 2 100001 shop_08 post 1 100001 shop_09 post 2 さて、ここでログデータお約束の｢ログがないデータは集計できない｣問題が発生します。例えば、｢1回目からshop_05に来たならば、その人のpreログデータは生成されていないので出てこない｣といった状況です。 この対処法はいくつかあるのですが、今回は面倒だったのでspreadするときにfill = 0として埋めることにしました: res &lt;- res %&gt;% spread(pre_post, count, fill = 0) knitr::kable(head(res, 10)) user_id shop_id post pre 100001 shop_01 5 0 100001 shop_02 2 0 100001 shop_03 2 0 100001 shop_04 3 0 100001 shop_05 5 0 100001 shop_06 2 1 100001 shop_07 2 0 100001 shop_08 1 0 100001 shop_09 2 0 100001 shop_10 2 0 これで完了です。 5.4 応用 今回のケースはズバリそのものというシチュエーションは少ないですが、これをベースにいろんな応用ができます。たとえば、｢shop_05への初回訪問以降、ある店を訪問した後に、次に訪問した店を把握できるようなデータがほしい｣という場合には、以下のようになります: df_5_fromto &lt;- df_5 %&gt;% # 並べ替えとユーザー単位でグループ化 arrange(user_id, dt) %&gt;% group_by(user_id) %&gt;% # フラグ立てとpre-postラベル付与 mutate(target_flag = if_else(shop_id == &quot;shop_05&quot;, 1, 0)) %&gt;% mutate(cum_target = cumsum(target_flag)) %&gt;% mutate(pre_post = if_else(cum_target == 0, &quot;pre&quot;, &quot;post&quot;)) %&gt;% # lead関数でひとつ下へずらす mutate(from_shop_id = shop_id, to_shop_id = lead(shop_id)) %&gt;% # preはいらんので取り除く filter(pre_post != &quot;pre&quot;) knitr::kable(head(df_5_fromto, 20)) user_id shop_id dt target_flag cum_target pre_post from_shop_id to_shop_id 100001 shop_05 2019-02-14 22:52:52 1 1 post shop_05 shop_05 100001 shop_05 2019-02-16 22:08:02 1 2 post shop_05 shop_03 100001 shop_03 2019-02-17 19:45:45 0 2 post shop_03 shop_07 100001 shop_07 2019-02-19 18:02:35 0 2 post shop_07 shop_04 100001 shop_04 2019-02-20 20:50:28 0 2 post shop_04 shop_10 100001 shop_10 2019-02-22 19:41:16 0 2 post shop_10 shop_03 100001 shop_03 2019-02-22 23:51:35 0 2 post shop_03 shop_01 100001 shop_01 2019-02-24 20:27:48 0 2 post shop_01 shop_09 100001 shop_09 2019-02-25 19:16:26 0 2 post shop_09 shop_06 100001 shop_06 2019-02-26 21:50:13 0 2 post shop_06 shop_01 100001 shop_01 2019-02-26 22:43:49 0 2 post shop_01 shop_06 100001 shop_06 2019-02-27 21:23:47 0 2 post shop_06 shop_09 100001 shop_09 2019-02-28 19:05:35 0 2 post shop_09 shop_10 100001 shop_10 2019-03-04 19:23:41 0 2 post shop_10 shop_05 100001 shop_05 2019-03-04 23:53:19 1 3 post shop_05 shop_07 100001 shop_07 2019-03-04 23:53:47 0 3 post shop_07 shop_05 100001 shop_05 2019-03-05 17:13:47 1 4 post shop_05 shop_04 100001 shop_04 2019-03-06 20:44:25 0 4 post shop_04 shop_04 100001 shop_04 2019-03-06 23:01:55 0 4 post shop_04 shop_01 100001 shop_01 2019-03-06 23:30:15 0 4 post shop_01 shop_02 あとは集計したりネットワーク分析をしたりと利活用できるでしょう。 5.5 参考資料 dplyr 関数リファレンス lubridate 関数リファレンス "]
]
